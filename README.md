
# Experiments with Explainable Deep Learning Models for MRI Imaging-Based Brain Tumor Classification

This project  “Experiments with Explainable Deep Learning Models for MRI Imaging-Based Brain Tumor Classification” Submitted by Nazim Uddin, Shahriar Ahmmed, Mosayeb Hossain, Md. Fahad Rana and Md Rokon Uzzaman students of the Department of Computer Science and Engineering, Bangladesh University of Business and Technology(BUBT), under the supervision of Md. Anwar Hussen Wadud, Assistant Professor, Department of Computer Science and Engineering has been accepted as satisfactory for the partial requirements for the degree of Bachelor of Science Engineering in Computer Science and Engineering. 


## Documentation

Clinical diagnosis now plays a vital part in today's healthcare system. In the realm of medical imaging, brain cancer is a major subject of study because it is the most deadly sickness and the leading cause of death globally. In order to assess the tumors and choose the best course of action for each kind of brain cancer, brain malignancies must be classified and located. Many imaging methods are used to detect brain tumors. MRI, on the other hand, is widely used since it emits no ionizing radiation and yields better images. Recently, the field of deep learning (DL) in machine learning has shown remarkable results, especially in classification and segmentation problems. Using an EfficientNetV2L convolutional neural network (CNN) and Shapley additive explanation (SHAP), we provide an explanation-driven Deep Learning model for the prediction of discrete subtypes of brain tumors (meningioma, glioma, and pituitary). An experimental study shows that the suggested approach for brain tumor detection and classification outperforms existing techniques in both visual and numerical domains, achieving a 99.96% accuracy rate. We next apply the Explainable Artificial Intelligence (XAI) technique (SHAP) to explain the result.
## Acknowledgements

First and foremost, we are grateful to Allah, the Almighty, the Merciful without whose patronage and blessing this project would not have been successfully completed. He gave us zeal, confidence, power of determination, and courage and vanquished all the stumbling hardness that we faced on the way. It is an auspicious occasion for us as students of the Department of Computer Science and Engineering, one of the prestigious academic centers of the Bangladesh University of Business and Technology (BUBT), to express our deep feelings of gratitude to the department and especially to our supervisor of the department, all the teachers and also to the departmental staff. We are immensely indebted to our supervisor, Md. Anwar Hussen Wadud, Assistant Professor, Department of Computer Science and Technology, for his wonderful guidance, inspiration, and encouragement and also for the thorough review and correction of this dissertation work that could not be finalized without his astute supervision. 
 
 
We pay profound regard to all of our teachers in the department for their very valuable directives and special attention. Our parents are very much keen and hopeful for the best performance of the dissertation we are going to submit. We wish we could fulfill their aspiration. We also pay regard to our friends in the department who, through their interest and work, are our contestant source of inspiration.  

	© Copyright by Shahriar Ahmmed, Md Rokon Uzzaman, Md. Fahad Rana, Mosayeb Hossain and Nazim Uddin bearing ID NO 19201103009, 19201103040, 19201103020, 19201103030 and 18192103054.

All Rights Reserved

## Introduction
This study examines the complex terrain of brain tumors, recognizing the critical need for a thorough knowledge and accurate diagnosis of a disease that has a significant impact on rates of morbidity and mortality in a variety of age groups, including adults and children. Because of their many different causes and unpredictable growth patterns, brain tumors present a very difficult challenge to the medical community that calls for a sophisticated strategy. Making the distinction between primary and secondary tumors helps to better understand the complex nature of this medical condition. Primary brain tumors, particularly malignant ones such as gliomas, are among the most formidable enemies in this field.

The astonishing advancements in medical imaging have brought about a new era of improved diagnosis accuracy, most notably the revolutionary influence of magnetic resonance imaging (MRI), which has made the terrain of controlling brain tumors slightly more manageable. Researchers are now more interested in solving the puzzles surrounding brain cancer, examining its complex developmental stages, and delving into its etiological roots thanks to the introduction of artificial intelligence, especially the powerful technology of deep learning.

However, one significant roadblock in this field is the transition from extremely accurate deep learning models to ones that are both interpretable and accurate. The use of Explainable AI (XAI) techniques has thus been strongly advocated, with the healthcare industry receiving special focus, in order to close the trust gap and strengthen the reliability of these advanced models. For this reason, the current work presents a painstakingly designed XAI framework intended to improve the clarity and openness of deep learning networks, with a focus on the demanding audience of medical practitioners. In general, the goal is to clarify the complex reasoning behind network predictions, which is a crucial aspect when dealing with delicate applications like brain imaging. The deliberate application of cutting-edge explaining techniques helps to accomplish this.

At the center of this effort is the creation of an explanation-driven deep learning model, a novel approach that uses the rich mosaic of MRI imaging data to predict various subtypes of brain tumors, including the dangerous trio of meningioma, glioma, and pituitary tumors. One key component of this approach is the model architecture's incorporation of the Shapley additive explanation (SHAP) method. The hallmark of SHAP is its careful examination of every possible set of input variables, which guarantees the consistency and local accuracy of the model's conclusions. The interpretability of the deep learning model is greatly enhanced by this methodological approach, which provides hitherto unseen insights into the complex interactions between specific features and how each affects the predictions of the model. Consequently, this refined comprehension strengthens the reliability of the model's results, which is crucial when making decisions about healthcare that have significant consequences.

## Problem Statement 
Indeed, because of their variety and possibly fatal consequences, brain tumors pose serious obstacles for both detection and treatment. Medical professionals continue to have concerns about the interpretability of deep learning models, despite the fact that imaging technology—particularly MRI scans—has significantly improved our ability to identify and comprehend these cancers. Although deep learning models have demonstrated encouraging results in the classification of brain tumor subtypes from magnetic resonance imaging (MRI), their interpretability issues may prevent their broad use in clinical practice. Interpretability, or the capacity to comprehend and elucidate a model's process of arriving at conclusions, is an essential skill in medical contexts where decisions have an effect on patient care. It is imperative to develop comprehensible and interpreted deep learning models especially for the purpose of identifying subtypes of brain tumors using MRI data. In addition to offering precise classifications, these models ought to give clinicians with justifications or illustrations for each diagnosis they receive. Data scientists and researchers are presently investigating ways to enhance the interpretability of deep learning models. The elements of the MRI scans that are impacting the model's decisions can be explained with the help of techniques like saliency maps, attention mechanisms, and visualizations of model activations. By developing understandable models, doctors may better trust and incorporate these AI tools into their decision-making processes, which improves the accuracy of brain tumor diagnosis. Thus, patients with brain tumors may benefit from more individualized and efficient treatment regimens.
## Problem Background

Because they significantly increase morbidity and mortality in all age groups, brain tumors pose a serious threat to public health. Whether they are secondary growths or primary tumors, accurate diagnosis is essential to developing successful treatment plans. Although there has been progress in the identification of brain cancers using MRI, deciphering MRI images is still a challenging endeavor. This intricacy is a major obstacle, impeding prompt and precise diagnosis, which is essential for timely therapies. Deep learning algorithms have potential to help identify brain tumors from MRI images. Nevertheless, one significant barrier to clinical integration is their opaqueness. Though these models could transform tumor identification and classification, their practical use in medical settings is limited by their lack of interpretability. The goal of Explainable AI (XAI) is to improve deep learning models' interpretability in order to close this gap. It becomes essential to create a XAI-driven framework designed especially for classifying brain tumors from MRI scans. Prioritizing accuracy enhancement while guaranteeing comprehensible and unambiguous insights for medical practitioners should be the hallmark of this approach. By giving these models interpretability, XAI hopes to provide doctors with understandable justifications for the judgments made by the model. This vital information helps medical professionals reliably use AI-driven insights to guide brain tumor patients' treatment decisions. Essentially, the search for a XAI-driven framework is a critical first step toward improving the ability to diagnose brain tumors from MRI scans. If it is successful, it will greatly increase accuracy, promote confidence among healthcare providers, and eventually enhance patient outcomes by supporting accurate and timely therapeutic treatments.
## Research Objectives 

This work aims to provide a deep learning framework that is explanation-driven by utilizing Shapley additive explanation (SHAP) and an EfficientNetv2L convolutional neural network (CNN). Using MRI imaging datasets, this system seeks to enable accurate categorization of distinct kinds of brain tumors, including meningioma, glioma, and pituitary. The project aims to improve the interpretability and transparency of deep learning models by incorporating state-of-the-art XAI approaches, hence offering insights into the reasoning behind predictions. The ultimate goal is to develop a very transparent and accurate model that end users and medical professionals can trust and understand, making it easier for clinical settings to adopt it to support decisions about brain tumor diagnosis and treatment.
## Authors

- [@RokonUzzaman](https://www.github.com/mrokonuzzaman040)

- [@ShahriarZe](https://www.github.com/ShahriarZe)
